"""
RLM æ·±åº¦åˆ†ææ¨¡çµ„
================
ä½¿ç”¨ Recursive Language Model å°æ–‡ç« é€²è¡Œæ·±åº¦å¤šè·³åˆ†æã€‚
"""

import asyncio
from typing import Optional
from loguru import logger

import sys
from pathlib import Path

# å°‡æœ¬åœ° lib/rlm åŠ å…¥è·¯å¾‘ (ç©©å®šå‚™æ¡ˆ)
lib_path = Path(__file__).parent.parent.parent / "lib" / "rlm"
if lib_path.exists() and str(lib_path) not in sys.path:
    sys.path.insert(0, str(lib_path))

try:
    from rlm.core.rlm import RLM
    from rlm.core.types import ClientBackend
    RLM_AVAILABLE = True
except ImportError as e:
    RLM_AVAILABLE = False
    # è©³ç´°è¨˜éŒ„éŒ¯èª¤ä»¥æ’é™¤ rich ç¼ºå¤±å•é¡Œ
    import traceback
    logger.warning(f"âš ï¸ RLM æ¨¡çµ„å°å…¥å¤±æ•—: {e}")
    traceback.print_exc()


class RLMAnalyzer:
    """RLM æ·±åº¦åˆ†æå™¨"""
    
    def __init__(self):
        if not RLM_AVAILABLE:
            raise RuntimeError("RLM æ¨¡çµ„æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: uv add rlm")
        
        # å¾ç’°å¢ƒè®Šæ•¸è®€å–é…ç½®ï¼Œæ”¯æ´ç›´é€£å¤–éƒ¨ API
        import os
        from dotenv import load_dotenv
        load_dotenv()

        base_url = os.getenv("API_BASE_URL", "http://localhost:3000/v1")
        api_key = os.getenv("API_KEY", "sk-antigravity-default")
        model_name = os.getenv("MODEL_NAME", "gemini-3-flash")

        self.rlm = RLM(
            backend="openai",
            backend_kwargs={
                "base_url": base_url,
                "api_key": api_key,
                "model_name": model_name
            },
            max_iterations=10,
            max_depth=1
        )

    async def analyze(self, title: str, summary: str, url: str) -> str:
        """å°æ–‡ç« é€²è¡Œæ·±åº¦åˆ†æ"""
        # Context è³‡è¨Š (æœƒè¢« RLM å­˜å…¥ context è®Šæ•¸)
        context_data = f"""è«–æ–‡æ¨™é¡Œ: {title}
æ‘˜è¦å…§å®¹: {summary}
åŸå§‹é€£çµ: {url}
"""

        # åˆ†ææŒ‡ä»¤ (å¼•å° RLM çš„ä¸»è¦ä»»å‹™)
        query = """è«‹é‡å°ä½ ç•¶å‰æä¾›çš„ contextï¼ˆè«–æ–‡æ‘˜è¦ï¼‰é€²è¡Œæ·±åº¦ä¸”çµæ§‹åŒ–çš„æã€‚
å¦‚æœä½ èªç‚ºç›®å‰è³‡è¨Šä¸è¶³ä»¥å›ç­”ä¸‹åˆ—å•é¡Œï¼Œè«‹å˜—è©¦ä½¿ç”¨ llm_query æ·±å…¥åˆ†æå…§å®¹ã€‚

åˆ†æç¶­åº¦å¦‚ä¸‹ï¼š
1. **æ ¸å¿ƒå‰µæ–°é»**ï¼šå…·é«”æŒ‡å‡ºè©²ç ”ç©¶åœ¨æŠ€è¡“æˆ–ç†è«–ä¸Šçš„çªç ´ã€‚
2. **æŠ€è¡“æ–¹æ³•**ï¼šè©³ç´°è§£æ§‹å…¶å¯¦ä½œè·¯å¾‘èˆ‡é—œéµæ¼”ç®—æ³•ã€‚
3. **æ‡‰ç”¨å ´æ™¯**ï¼šé™¤äº†ä½œè€…æåˆ°çš„ï¼Œé‚„æœ‰å“ªäº›æ½›åœ¨çš„è¡Œæ¥­æ‡‰ç”¨ï¼Ÿ
4. **æ½›åœ¨å½±éŸ¿**ï¼šè©•åƒ¹å…¶å°ç¾æœ‰æŠ€è¡“ç”Ÿæ…‹çš„é•·é å½±éŸ¿ã€‚
5. **ç›¸é—œå·¥ä½œ**ï¼šè¯æƒ³ä¸¦åˆ—å‡ºç›¸é—œçš„æŠ€è¡“è¶¨å‹¢æˆ–ç ”ç©¶ã€‚

è«‹æ³¨æ„ï¼š
- è¼¸å‡ºå¿…é ˆæ˜¯ç¹é«”ä¸­æ–‡ã€‚
- å…§å®¹è¦æ·±å…¥ä¸”å°ˆæ¥­ï¼Œé¿å…ç©ºæ´çš„å»¢è©±ã€‚
- ç•¶ä½ å®Œæˆæ‰€æœ‰åˆ†æå¾Œï¼Œè«‹ä½¿ç”¨ FINAL(åˆ†æå…§å®¹) æä¾›æœ€çµ‚ç‰ˆæœ¬ã€‚"""
        
        logger.info(f"ğŸ”¬ é–‹å§‹ RLM æ·±åº¦åˆ†æ (å„ªåŒ–ç‰ˆ): {title[:50]}...")
        
        try:
            # ä½¿ç”¨ root_prompt åƒæ•¸ä¾†åˆ†é›¢æŒ‡ä»¤èˆ‡è³‡æ–™
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.rlm.completion(prompt=context_data, root_prompt=query)
            )
            logger.success(f"âœ… RLM åˆ†æå®Œæˆ: {title[:50]}...")
            return result.response
        except Exception as e:
            logger.error(f"âŒ RLM åˆ†æå¤±æ•—: {e}")
            return f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}"


# å–®ä¾‹å¯¦ä¾‹
_analyzer: Optional[RLMAnalyzer] = None


def get_analyzer() -> RLMAnalyzer:
    """ç²å– RLM åˆ†æå™¨å¯¦ä¾‹"""
    global _analyzer
    if _analyzer is None:
        _analyzer = RLMAnalyzer()
    return _analyzer
