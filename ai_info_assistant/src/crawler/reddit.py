"""
Reddit RSS çˆ¬èŸ²
================
ä½¿ç”¨ Reddit å…¬é–‹ RSS Feed æŠ“å– subreddit ç†±é–€æ–‡ç« ã€‚
ä¸éœ€è¦ API èªè­‰ï¼Œä½¿ç”¨ feedparser è§£æ RSSã€‚
"""

import feedparser
from datetime import datetime, timezone
from typing import List
from .base import BaseCrawler
from ..models import Article
from loguru import logger


class RedditCrawler(BaseCrawler):
    """Reddit RSS çˆ¬èŸ² - å¾æŒ‡å®š subreddits æŠ“å–ç†±é–€æ–‡ç« """
    
    RSS_TEMPLATE = "https://www.reddit.com/r/{subreddit}/{sort}.rss"

    async def fetch(self) -> List[Article]:
        subreddits = self.config.params.get("subreddits", ["MachineLearning"])
        max_items = self.config.params.get("max_items", 10)
        sort = self.config.params.get("sort", "hot")  # hot, new, top
        
        logger.debug(f"ğŸ” è«‹æ±‚ Reddit RSS ({len(subreddits)} subreddits, sort={sort})")
        
        articles = []
        items_per_sub = max(1, max_items // len(subreddits))
        
        for subreddit in subreddits:
            try:
                url = self.RSS_TEMPLATE.format(subreddit=subreddit, sort=sort)
                feed = feedparser.parse(url)
                
                if feed.bozo and not feed.entries:
                    logger.warning(f"âš ï¸ Reddit RSS è§£æå¤±æ•—: r/{subreddit}")
                    continue
                
                for entry in feed.entries[:items_per_sub]:
                    try:
                        # è§£æç™¼å¸ƒæ™‚é–“
                        published = entry.get("published_parsed") or entry.get("updated_parsed")
                        if published:
                            dt = datetime(*published[:6], tzinfo=timezone.utc)
                        else:
                            dt = datetime.now(timezone.utc)
                        
                        # å¾ entry.id å–å¾— Reddit post ID
                        entry_id = entry.get("id", entry.get("link", ""))
                        post_id = entry_id.split("/")[-2] if "/comments/" in entry_id else entry_id[-8:]
                        
                        article = Article(
                            id=f"reddit-{subreddit.lower()}-{post_id}",
                            title=self.clean_text(entry.get("title", "")),
                            authors=[entry.get("author", f"r/{subreddit}")],
                            summary=self._extract_summary(entry),
                            url=entry.get("link", ""),
                            source="reddit",
                            published_date=dt
                        )
                        articles.append(article)
                    except Exception as e:
                        logger.warning(f"âš ï¸ è§£æ Reddit é …ç›®å¤±æ•—: {e}")
                        continue
                
                logger.debug(f"âœ… r/{subreddit}: å–å¾— {min(len(feed.entries), items_per_sub)} ç¯‡")
                
            except Exception as e:
                logger.error(f"âŒ æŠ“å– r/{subreddit} å¤±æ•—: {e}")
                continue
        
        logger.debug(f"âœ… Reddit å…±å–å¾— {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    
    def _extract_summary(self, entry) -> str:
        """å¾ RSS entry æå–æ‘˜è¦"""
        # RSS content é€šå¸¸åŒ…å« HTMLï¼Œå˜—è©¦å–å¾—ç´”æ–‡å­—
        summary = entry.get("summary", "")
        if not summary:
            content = entry.get("content", [])
            if content and isinstance(content, list):
                summary = content[0].get("value", "")
        
        # ç°¡å–®ç§»é™¤ HTML tags
        import re
        summary = re.sub(r'<[^>]+>', ' ', summary)
        summary = re.sub(r'\s+', ' ', summary).strip()
        
        # é™åˆ¶é•·åº¦
        if len(summary) > 500:
            summary = summary[:497] + "..."
        
        return summary if summary else "Reddit è¨è«–æ–‡ç« "
