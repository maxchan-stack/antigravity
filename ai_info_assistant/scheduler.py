#!/usr/bin/env python3
"""
AI è³‡è¨ŠåŠ©ç† - å®šæ™‚æ’ç¨‹è…³æœ¬
==========================
è‡ªå‹•å®šæ™‚åŸ·è¡Œè³‡è¨ŠæŠ“å–ã€è©•åˆ†èˆ‡æç…‰ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    uv run scheduler.py              # ä½¿ç”¨ .env ä¸­çš„é–“éš”è¨­å®š (é è¨­ 2 å°æ™‚)
    uv run scheduler.py --interval 60  # æ¯ 60 åˆ†é˜åŸ·è¡Œä¸€æ¬¡
    uv run scheduler.py --once         # åªåŸ·è¡Œä¸€æ¬¡
"""

import asyncio
import argparse
from datetime import datetime
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from loguru import logger

from src.crawler.arxiv import ArxivCrawler
from src.crawler.github import GithubCrawler
from src.crawler.hackernews import HackerNewsCrawler
from src.crawler.reddit import RedditCrawler
from src.scoring.engine import ScoringEngine
from src.scoring.relevance import RelevanceScorer
from src.refiner.engine import RefinerEngine
from src.database.storage import DatabaseManager
from src.models import SourceConfig
from src.config import config


CRAWLER_MAP = {
    "arxiv": ArxivCrawler,
    "github": GithubCrawler,
    "hn": HackerNewsCrawler,
    "reddit": RedditCrawler
}


async def run_pipeline():
    """åŸ·è¡Œå®Œæ•´çš„æŠ“å–-è©•åˆ†-æç…‰æµç¨‹"""
    start_time = datetime.now()
    logger.info(f"ğŸš€ [{start_time.strftime('%H:%M:%S')}] é–‹å§‹åŸ·è¡Œè³‡è¨Šæ“·å–æµç¨‹...")
    
    # åˆå§‹åŒ–
    db = DatabaseManager()
    processed_ids = db.get_processed_ids()
    logger.info(f"ğŸ“Š è³‡æ–™åº«å·²æœ‰ {len(processed_ids)} æ¢è¨˜éŒ„")

    # 1. å®šç¾©è³‡è¨Šæºé…ç½® (ä½¿ç”¨ config.py çš„è¨­å®š)
    configs = [
        SourceConfig(name="arxiv", params={"query": "cat:cs.AI OR cat:cs.LG", "max_results": config.arxiv_max_results}),
        SourceConfig(name="github", params={"language": config.github_language}),
        SourceConfig(name="hn", params={"max_items": config.hn_max_items}),
        SourceConfig(name="reddit", params={"subreddits": config.reddit_subreddits, "max_items": config.reddit_max_items})
    ]
    
    # 2. æŠ“å–ä¸¦éæ¿¾
    raw_articles = []
    for cfg in configs:
        crawler_cls = CRAWLER_MAP.get(cfg.name)
        if not crawler_cls:
            continue
        crawler = crawler_cls(cfg)
        fetched = await crawler.fetch()
        
        # é—œéµå­—éæ¿¾
        filtered = [a for a in fetched if config.matches_interests(a.title + " " + a.summary)]
        new_items = [a for a in filtered if a.id not in processed_ids]
        raw_articles.extend(new_items)
        logger.info(f"ğŸ“¥ {cfg.name}: æŠ“å– {len(fetched)} -> é—œéµå­—éæ¿¾ {len(filtered)} -> æ–°å…§å®¹ {len(new_items)}")
    
    if not raw_articles:
        logger.success("âœ¨ æ²’æœ‰æ–°çš„ç›¸é—œå…§å®¹ï¼")
        return

    # 3. å‚³çµ±è©•åˆ†
    scoring_engine = ScoringEngine()
    scored_articles = scoring_engine.process_articles(raw_articles)
    
    # 4. LLM äºŒéšæ®µç›¸é—œæ€§è©•åˆ† (åªè™•ç†å‰ 10 å)
    relevance_scorer = RelevanceScorer()
    top_candidates = scored_articles[:10]
    relevance_results = await relevance_scorer.score_batch(top_candidates)
    relevant_articles = relevance_scorer.filter_relevant(relevance_results)
    
    if not relevant_articles:
        logger.info("âš ï¸ æ²’æœ‰æ–‡ç« é€šéç›¸é—œæ€§è©•åˆ†")
        return

    # 5. æ·±åº¦æç…‰
    refiner = RefinerEngine()
    refined_articles = await refiner.batch_refine(relevant_articles, top_n=len(relevant_articles))
    
    # 6. ä¿å­˜
    db.save_articles(refined_articles)
    
    elapsed = (datetime.now() - start_time).total_seconds()
    logger.success(f"ğŸ‰ æµç¨‹å®Œæˆï¼æç…‰ {len(refined_articles)} ç¯‡ï¼Œè€—æ™‚ {elapsed:.1f}s")


def main():
    parser = argparse.ArgumentParser(description="AI è³‡è¨ŠåŠ©ç†å®šæ™‚æ’ç¨‹")
    parser.add_argument("--interval", type=int, default=config.schedule_interval_mins, help="åŸ·è¡Œé–“éš” (åˆ†é˜)")
    parser.add_argument("--once", action="store_true", help="åªåŸ·è¡Œä¸€æ¬¡")
    args = parser.parse_args()
    
    if args.once:
        logger.info("ğŸ”„ å–®æ¬¡åŸ·è¡Œæ¨¡å¼")
        asyncio.run(run_pipeline())
        return
    
async def start_scheduler(interval_mins: int):
    """å•Ÿå‹•éåŒæ­¥æ’ç¨‹å™¨"""
    scheduler = AsyncIOScheduler()
    scheduler.add_job(
        run_pipeline,
        trigger=IntervalTrigger(minutes=interval_mins),
        id="info_pipeline",
        name="è³‡è¨Šæ“·å–æµç¨‹",
        next_run_time=datetime.now()  # ç«‹å³åŸ·è¡Œä¸€æ¬¡
    )
    
    scheduler.start()
    logger.info(f"â° å•Ÿå‹•å®šæ™‚æ’ç¨‹ï¼Œæ¯ {interval_mins} åˆ†é˜åŸ·è¡Œä¸€æ¬¡")
    logger.info("   æŒ‰ Ctrl+C åœæ­¢")
    
    try:
        while True:
            await asyncio.sleep(3600)  # ä¿æŒå¾ªç’°é‹è¡Œ
    except (KeyboardInterrupt, SystemExit):
        logger.info("ğŸ‘‹ æ’ç¨‹å·²åœæ­¢")
        scheduler.shutdown()


def main():
    parser = argparse.ArgumentParser(description="AI è³‡è¨ŠåŠ©ç†å®šæ™‚æ’ç¨‹")
    parser.add_argument("--interval", type=int, default=config.schedule_interval_mins, help="åŸ·è¡Œé–“éš” (åˆ†é˜)")
    parser.add_argument("--once", action="store_true", help="åªåŸ·è¡Œä¸€æ¬¡")
    args = parser.parse_args()
    
    if args.once:
        logger.info("ğŸ”„ å–®æ¬¡åŸ·è¡Œæ¨¡å¼")
        asyncio.run(run_pipeline())
        return
    
    try:
        asyncio.run(start_scheduler(args.interval))
    except (KeyboardInterrupt, SystemExit):
        pass


if __name__ == "__main__":
    main()
