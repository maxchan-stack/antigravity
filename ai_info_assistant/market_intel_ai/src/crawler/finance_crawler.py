import asyncio
import feedparser
import httpx
from typing import List
from datetime import datetime
from email.utils import parsedate_to_datetime
from loguru import logger
from .base import BaseCrawler
from ..models import Article, SourceConfig

class FinancialCrawler(BaseCrawler):
    """è²¡ç¶“æ–°èçˆ¬èŸ² (Google Finance RSS)"""
    
    BASE_URL = "https://news.google.com/rss/search"

    async def fetch(self) -> List[Article]:
        tickers = self.config.params.get("tickers", [])
        if not tickers:
            logger.warning("æœªè¨­å®šè‚¡ç¥¨ä»£è™Ÿ")
            return []

        all_articles = []
        for ticker in tickers:
            articles = await self._fetch_ticker(ticker)
            all_articles.extend(articles)
        
        return all_articles

    async def _fetch_ticker(self, ticker: str) -> List[Article]:
        params = {
            "q": f"{ticker} stock",
            "hl": "en-US",
            "gl": "US",
            "ceid": "US:en"
        }
        
        try:
            logger.info(f"ğŸ” æ­£åœ¨æŠ“å– {ticker} çš„è²¡ç¶“æ–°è...")
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(self.BASE_URL, params=params)
                response.raise_for_status()
                
            feed = feedparser.parse(response.content)
            articles = []
            
            for entry in feed.entries[:5]:  # æ¯å€‹ä»£è™Ÿåªå–å‰ 5 å‰‡
                try:
                    pub_date = datetime.now()
                    if hasattr(entry, "published"):
                        pub_date = parsedate_to_datetime(entry.published)
                        
                    article = Article(
                        id=entry.link,
                        title=f"[{ticker}] {entry.title}",
                        authors=[entry.source.title] if hasattr(entry, "source") else [],
                        summary=entry.description if hasattr(entry, "description") else "",
                        url=entry.link,
                        source="finance",
                        published_date=pub_date,
                        trust_score=0.0,  # å¾… AI è©•åˆ†
                        tags=[ticker, "Stock"]
                    )
                    articles.append(article)
                except Exception as e:
                    logger.warning(f"è§£ææ–°èæ¢ç›®å¤±æ•—: {e}")
                    continue
                    
            logger.success(f"âœ… {ticker}: å–å¾— {len(articles)} å‰‡æ–°è")
            return articles
            
        except Exception as e:
            logger.error(f"âŒ æŠ“å– {ticker} å¤±æ•—: {e}")
            return []
