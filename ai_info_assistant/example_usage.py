import asyncio
from src.crawler.arxiv import ArxivCrawler
from src.crawler.github import GithubCrawler
from src.crawler.hackernews import HackerNewsCrawler
from src.scoring.engine import ScoringEngine
from src.refiner.engine import RefinerEngine
from src.database.storage import DatabaseManager
from src.models import SourceConfig
from loguru import logger

# çˆ¬èŸ²é¡åˆ¥æ˜ å°„
CRAWLER_MAP = {
    "arxiv": ArxivCrawler,
    "github": GithubCrawler,
    "hn": HackerNewsCrawler
}

async def main():
    logger.info("ğŸš€ å•Ÿå‹• AI è³‡è¨ŠåŠ©ç†æœ€çµ‚å±•ç¤º ( Grand Finale )...")
    
    # åˆå§‹åŒ–è³‡æ–™åº«
    db = DatabaseManager()
    processed_ids = db.get_processed_ids()
    logger.info(f"ğŸ“Š ç›®å‰è³‡æ–™åº«ä¸­å·²æœ‰ {len(processed_ids)} æ¢è¨˜éŒ„")

    # 1. å®šç¾©è³‡è¨Šæºé…ç½® (æ–°å¢ HN)
    configs = [
        SourceConfig(name="arxiv", params={"query": "cat:cs.AI", "max_results": 5}),
        SourceConfig(name="github", params={"language": "python"}),
        SourceConfig(name="hn", params={"max_items": 10})
    ]
    
    # 2. æŠ“å–ä¸¦éæ¿¾
    raw_articles = []
    
    for config in configs:
        crawler_cls = CRAWLER_MAP.get(config.name)
        if not crawler_cls:
            continue
        crawler = crawler_cls(config)
        fetched = await crawler.fetch()
        new_items = [a for a in fetched if a.id not in processed_ids]
        raw_articles.extend(new_items)
        logger.info(f"ğŸ“¥ {config.name}: ç²å¾— {len(fetched)} æ¢ï¼Œå…¶ä¸­ {len(new_items)} æ¢ç‚ºæ–°å…§å®¹")
    
    if not raw_articles:
        logger.success("âœ¨ æ²’æœ‰æ–°çš„å…§å®¹éœ€è¦è™•ç†ï¼")
        return

    # 3. è©•åˆ†
    scoring_engine = ScoringEngine()
    scored_articles = scoring_engine.process_articles(raw_articles)
    
    # 4. æç…‰ (åˆ†ä¾†æºå–æ¨£ï¼Œç¢ºä¿å¤šæ¨£æ€§ï¼šæ¯å€‹ä¾†æºæœ€å¤š 2 ç¯‡)
    from collections import defaultdict
    articles_by_source = defaultdict(list)
    for art in scored_articles:
        articles_by_source[art.source].append(art)
    
    to_refine = []
    for source, arts in articles_by_source.items():
        to_refine.extend(arts[:2])  # æ¯å€‹ä¾†æºå–å‰ 2 å
    
    refiner = RefinerEngine()
    refined_articles = await refiner.batch_refine(to_refine, top_n=len(to_refine))
    
    # 5. ä¿å­˜çµæœ
    db.save_articles(refined_articles)
    
    logger.info(f"âœ… å±•ç¤ºæœ¬è¼ªæç…‰çš„ Top {len(refined_articles)} çŸ¥è­˜:")
    
    for i, art in enumerate(refined_articles):
        source_emoji = {"arxiv": "ğŸ“„", "github": "ğŸ› ï¸", "hn": "ğŸ”¥"}.get(art.source, "ğŸ“Œ")
        logger.info(f"{source_emoji} [Top {i+1}] {art.title} (Score: {art.trust_score})")
        logger.info(f"   æ¨™ç±¤: {', '.join(art.tags)}")
        logger.info(f"   ğŸ’¡ AI æ‘˜è¦: {art.ai_summary}")
        logger.info("-" * 20)

if __name__ == "__main__":
    asyncio.run(main())
